Host：用于指定请求资源的主机IP和端口号，其内容为请求URL的原始服务器或网关的位置。从HTTP 1.1版本开始，请求必须包含此内容。
User-Agent：简称UA，它是一个特殊的字符串头，可以使服务器识别客户使用的操作系统及版本、浏览器及版本等信息。在做爬虫时加上此信息，可以伪装为浏览器；如果不加，很可能会被识别出为爬虫。

常见的请求方法有两种：GET和POST

GET和POST请求方法有如下区别：
1.GET请求中的参数包含在URL里面，数据可以在URL中看到，而POST请求的URL不会包含这些数据，数据都是通过表单形式传输的，会包含在请求体中。
2.GET请求提交的数据最多只有1024字节，而POST方式没有限制。
一般来说，登录时，需要提交用户名和密码，其中包含了敏感信息，使用GET方式请求的话，密码就会暴露在URL里面，造成密码泄露，所以这里最好以POST方式发送。上传文件时，由于文件内容比较大，也会选用POST方式。

方法|描述
:---:|:---:
GET|请求页面，并返回页面内容
HEAD|类似于GET请求，只不过返回的响应中没有具体的内容，用于获取报头
POST|大多用于提交表单或上传文件，数据包含在请求体中
PUT|从客户端向服务器传送的数据取代指定文档中的内容
DELETE|请求服务器删除指定的页面
CONNECT|把服务器当作跳板，让服务器代替客户端访问其他网页
OPTIONS|允许客户端查看服务器的性能
TRACE|回显服务器收到的请求，主要用于测试或诊断


**最简单的爬网页方式：**
（```）
import urllib.request
response = urllib.request.urlopen('https://www.python.org')
print(response.read().decode('utf-8'))

#看它返回的到底是什么。利用type()方法输出响应的类型
print(type(response))
#调用read()方法可以得到返回的网页内容，调用status属性可以得到返回结果的状态码，如200代表请求成功，404代表网页未找到等。
print(response.status)
print(response.getheaders())
print(response.getheader('Server'))
#前两个输出分别输出了响应的状态码和响应的头信息，最后一个输出通过调用getheader()方法并传递一个参数Server获取了响应头中的Server值，结果是nginx，意思是服务器是用Nginx搭建的。
(```)

**利用request爬取网页：**
(```)
import urllib.request
request=urllib.request.Request('http://www.python.org')
response=urllib.request.urlopen(request)
print(response.read().decode('utf-8'))
(```)

**Request的构造方法：**
class urllib.request.Request(url, data=None, headers={}, origin_req_host=None, unverifiable=False, method=None)
第一个参数url用于请求URL，这是必传参数，其他都是可选参数。
第二个参数data如果要传，必须传bytes（字节流）类型的。如果它是字典，可以先用urllib.parse模块里的urlencode()编码。
第三个参数headers是一个字典，它就是请求头，我们可以在构造请求时通过headers参数直接构造，也可以通过调用请求实例的add_header()方法添加。
添加请求头最常用的用法就是通过修改User-Agent来伪装浏览器，默认的User-Agent是Python-urllib，我们可以通过修改它来伪装浏览器。比如要伪装火狐浏览器，你可以把它设置为：
Mozilla/5.0 (X11; U; Linux i686) Gecko/20071127 Firefox/2.0.0.11

第四个参数origin_req_host指的是请求方的host名称或者IP地址。
第五个参数unverifiable表示这个请求是否是无法验证的，默认是False，意思就是说用户没有足够权限来选择接收这个请求的结果。例如，我们请求一个HTML文档中的图片，但是我们没有自动抓取图像的权限，这时unverifiable的值就是True。
第六个参数method是一个字符串，用来指示请求使用的方法，比如GET、POST和PUT等。

**多个数据构建请求（request）**
(```)
from urllib import request, parse
 url = 'http://httpbin.org/post'
headers = {
    'User-Agent': 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)',
    'Host': 'httpbin.org'
}
dict = {
    'name': 'Germey'
}
data = bytes(parse.urlencode(dict), encoding='utf8')
req = request.Request(url=url, data=data, headers=headers, method='POST')
response = request.urlopen(req)
print(response.read().decode('utf-8'))
(```)

另外，headers也可以用add_header()方法来添加：
`req = request.Request(url=url, data=data, method='POST')
req.add_header('User-Agent', 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)')`

**URLError**
它来自urllib库的error模块，它具有一个属性reason，即返回错误的原因
(```)
from urllib import request, error
try:
    response = request.urlopen('http://cuiqingcai.com/index.htm')
except error.URLError as e:
    print(e.reason)
(```)
我们打开一个不存在的页面，照理来说应该会报错，但是这时我们捕获了URLError这个异常，运行结果如下：
`Not Found`

**HTTPError**
它是URLError的子类，专门用来处理HTTP请求错误，有三个属性
code：返回HTTP状态码，比如404表示网页不存在，500表示服务器内部错误等。
reason：同父类一样，用于返回错误的原因。
headers：返回请求头。
(```)
from urllib import request,error
try:
    response = request.urlopen('http://cuiqingcai.com/index.htm')
except error.HTTPError as e:
    print(e.reason, e.code, e.headers, sep='\n')
(```)
运行结果如下：
(```)
Not Found
404
Server: nginx/1.4.6 (Ubuntu)
Date: Wed, 03 Aug 2016 08:54:22 GMT
Content-Type: text/html; charset=UTF-8
Transfer-Encoding: chunked
Connection: close
X-Powered-By: PHP/5.5.9-1ubuntu4.14
Vary: Cookie
Expires: Wed, 11 Jan 1984 05:00:00 GMT
Cache-Control: no-cache, must-revalidate, max-age=0
Pragma: no-cache
Link: <http://cuiqingcai.com/wp-json/>; rel="https://api.w.org/"
(```)


因为URLError是HTTPError的父类，所以可以先选择捕获子类的错误，再去捕获父类的错误，例如：
(```)
from urllib import request, error
 
try:
    response = request.urlopen('http://cuiqingcai.com/index.htm')
except error.HTTPError as e:
    print(e.reason, e.code, e.headers, sep='\n')
except error.URLError as e:
    print(e.reason)
else:
    print('Request Successfully')
(```)
这样就可以做到先捕获HTTPError，获取它的错误状态码、原因、headers等信息。如果不是HTTPError异常，就会捕获URLError异常，输出错误原因。最后，用else来处理正常的逻辑。这是一个较好的异常处理写法。


**prase**
利用urlparse()方法进行了一个URL的解析
```
from urllib.parse import urlparse
result = urlparse('http://www.baidu.com/index.html;user?id=5#comment')
print(type(result), result)
```
返回结果为：
```
<class 'urllib.parse.ParseResult'>
ParseResult(scheme='http', netloc='www.baidu.com', path='/index.html', params='user', query='id=5', fragment='comment')
```
即urlparse()方法将其拆分成了6部分。大体观察可以发现，解析时有特定的分隔符。比如，://前面的就是scheme，代表协议；第一个/前面便是netloc，即域名；分号;前面是params，代表参数。

urlparse()API用法：
`urllib.parse.urlparse(urlstring, scheme='', allow_fragments=True)`
urlstring：这是必填项，即待解析的URL。

**scheme**：它是默认的协议（比如http或https等）。假如这个链接没有带协议信息，会将这个作为默认的协议。
```
from urllib.parse import urlparse
result = urlparse('www.baidu.com/index.html;user?id=5#comment', scheme='https')
print(result)
```
可以发现，我们提供的URL没有包含最前面的scheme信息，但是通过指定默认的scheme参数，返回的结果是https。
`ParseResult(scheme='https', netloc='', path='www.baidu.com/index.html', params='user', query='id=5', fragment='comment')`

当不带scheme时：
`result = urlparse('http://www.baidu.com/index.html;user?id=5#comment')`
结果为：
`ParseResult(scheme='http', netloc='www.baidu.com', path='/index.html', params='user', query='id=5', fragment='comment')'`

由此，可发现：scheme参数只有在URL中不包含scheme信息时才生效。如果URL中有scheme信息，就会返回解析出的scheme。

**allow_fragments**：即是否忽略fragment。如果它被设置为False，fragment部分就会被忽略，它会被解析为path、parameters或者query的一部分，而fragment部分为空。
```
from urllib.parse import urlparse
result = urlparse('http://www.baidu.com/index.html;user?id=5#comment', allow_fragments=False)
print(result)

ParseResult(scheme='http', netloc='www.baidu.com', path='/index.html', params='user', query='id=5#comment', fragment='')
```
返回结果ParseResult实际上是一个元组，我们可以用索引顺序来获取，也可以用属性名获取。示例如下：
```
from urllib.parse import urlparse
result = urlparse('http://www.baidu.com/index.html#comment', allow_fragments=False)
print(result.scheme, result[0], result.netloc, result[1], sep='\n')
```
这里我们分别用索引和属性名获取了scheme和netloc，其运行结果如下：
```
http
http
www.baidu.com
www.baidu.com
```



